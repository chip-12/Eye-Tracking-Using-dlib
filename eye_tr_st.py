#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().run_cell_magic('writefile', 'hello_world.py', '\nimport streamlit as st\nimport cv2\nimport numpy as np\nimport pyautogui\nimport time\nimport dlib\nfrom imutils import face_utils\n\n# Initialize variables to store previous pupil positions and wink detection time\nprev_left_pupil_center = None\nprev_right_pupil_center = None\nwink_detected_time = 0\nwink_duration = 0.5  # Duration in seconds for detecting a wink (adjusted)\nscroll_active = False  # Flag to indicate if scrolling is active\n\n# Initialize variables for eye tracking data smoothing\nsmoothing_window_size = 5\nleft_eye_positions = []\nright_eye_positions = []\n\n# Function to center the mouse cursor on the screen\ndef center_cursor():\n    screen_width, screen_height = pyautogui.size()\n    pyautogui.moveTo(screen_width // 2, screen_height // 2)\n\n# Function to smooth eye tracking data using a moving average filter\ndef smooth_eye_positions(left_eye_center, right_eye_center):\n    global left_eye_positions, right_eye_positions\n\n    # Add current eye positions to the lists\n    left_eye_positions.append(left_eye_center)\n    right_eye_positions.append(right_eye_center)\n\n    # Truncate the lists if they exceed the smoothing window size\n    if len(left_eye_positions) > smoothing_window_size:\n        left_eye_positions = left_eye_positions[-smoothing_window_size:]\n    if len(right_eye_positions) > smoothing_window_size:\n        right_eye_positions = right_eye_positions[-smoothing_window_size:]\n\n    # Calculate the average eye positions\n    smoothed_left_eye_center = np.mean(left_eye_positions, axis=0)\n    smoothed_right_eye_center = np.mean(right_eye_positions, axis=0)\n\n    return smoothed_left_eye_center, smoothed_right_eye_center\n\n# Function to calculate vertical eye movement\ndef calculate_vertical_movement(left_eye_center):\n    global prev_left_pupil_center\n\n    # Calculate the vertical distance between current and previous eye positions\n    if prev_left_pupil_center is not None:\n        vertical_movement = left_eye_center[1] - prev_left_pupil_center[1]\n        return vertical_movement\n    else:\n        return 0\n\n# Function to move cursor based on eye movement\ndef move_cursor(left_pupil_center):\n    global prev_left_pupil_center\n\n    # Calculate the direction of eye movement based on pupil positions\n    if prev_left_pupil_center is not None:\n        eye_direction_x = left_pupil_center[0] - prev_left_pupil_center[0]\n        eye_direction_y = left_pupil_center[1] - prev_left_pupil_center[1]\n\n        # Move the cursor\n        move_x, move_y = -eye_direction_x, eye_direction_y\n        pyautogui.moveRel(move_x * 5, move_y * 5, duration=0.01)\n\n    # Update previous pupil position\n    prev_left_pupil_center = left_pupil_center\n\n# Function to check for wink detection and perform scroll action\ndef check_wink_detection(shape):\n    global wink_detected_time, scroll_active\n\n    # Find the center of the right eye\n    right_eye_pts = shape[42:48]\n    right_eye_center = right_eye_pts.mean(axis=0).astype(int)\n\n    # Find the center of the right pupil\n    right_pupil_center = shape[43].astype(int)\n\n    # Detect wink based on eye aspect ratio\n    right_ear = eye_aspect_ratio(right_eye_pts)\n    if right_ear < 0.2 and wink_detected_time == 0:\n        wink_detected_time = time.time()\n        st.write("Wink detected!")\n\n    # Check if wink duration exceeds threshold\n    if wink_detected_time != 0 and time.time() - wink_detected_time >= wink_duration:\n        if right_ear < 0.2:\n            scroll_active = not scroll_active\n            wink_detected_time = 0\n            st.write("Scroll activated!" if scroll_active else "Scroll deactivated!")\n\n    # Perform scroll action if scrolling is active\n    if scroll_active:\n        # Calculate vertical eye movement for scrolling\n        vertical_movement = calculate_vertical_movement(right_pupil_center)\n\n        # Determine scrolling direction\n        if vertical_movement > 0:\n            pyautogui.scroll(2)  # Scroll up\n            st.write("Scrolling up...")\n        elif vertical_movement < 0:\n            pyautogui.scroll(-2)  # Scroll down\n            st.write("Scrolling down...")\n\n# Function to check for blink detection and perform click action\ndef check_blink_detection(left_eye_pts, right_eye_pts):\n    left_ear = eye_aspect_ratio(left_eye_pts)\n    right_ear = eye_aspect_ratio(right_eye_pts)\n\n    # Detect blink based on eye aspect ratio\n    if left_ear < 0.2 and right_ear < 0.2:\n        time.sleep(0.2)\n        # Perform click action for blink\n        pyautogui.click()\n        st.write("Blink detected and click performed!")\n        return True\n    else:\n        return False\n\n# Function to calculate eye aspect ratio\ndef eye_aspect_ratio(eye):\n    A = np.linalg.norm(eye[1] - eye[5])\n    B = np.linalg.norm(eye[2] - eye[4])\n    C = np.linalg.norm(eye[0] - eye[3])\n    ear = (A + B) / (2.0 * C)\n    return ear\n\nimport io\n\n# Function to run eye tracking and control\ndef run_eye_tracking():\n    # Initialize dlib\'s face detector (HOG-based) and create the facial landmark predictor\n    p = "/Users/nandinidhiran/Downloads/shape_predictor_68_face_landmarks.dat"  \n    detector = dlib.get_frontal_face_detector()\n    predictor = dlib.shape_predictor(p)\n\n    # Open the camera capture\n    cap = cv2.VideoCapture(0)\n\n    # Center the mouse cursor on the screen\n    center_cursor()\n\n    # Create a Streamlit placeholder for displaying the image\n    image_placeholder = st.empty()\n\n    while True:\n        # Read frame from camera\n        _, image = cap.read()\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Detect faces in the grayscale image\n        rects = detector(gray, 0)\n\n        # Loop over the face detections\n        for rect in rects:\n            # Determine the facial landmarks for the face region\n            shape = predictor(gray, rect)\n            shape = face_utils.shape_to_np(shape)\n\n            # Draw facial landmarks on the image\n            for (x, y) in shape:\n                cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n\n            # Find the center of the left eye\n            left_eye_pts = shape[36:42]\n            left_eye_center = left_eye_pts.mean(axis=0).astype(int)\n\n            # Find the center of the left pupil\n            left_pupil_center = shape[37].astype(int)\n\n            # Find the center of the right eye\n            right_eye_pts = shape[42:48]\n            right_eye_center = right_eye_pts.mean(axis=0).astype(int)\n\n            # Find the center of the right pupil\n            right_pupil_center = shape[43].astype(int)\n\n            # Smooth eye positions\n            smoothed_left_eye_center, smoothed_right_eye_center = smooth_eye_positions(left_eye_center, right_eye_center)\n\n            # Move cursor based on smoothed eye movement\n            move_cursor(smoothed_left_eye_center)\n\n            # Check for blink detection and perform click action\n            if check_blink_detection(left_eye_pts, right_eye_pts):\n                continue  # Skip wink detection if blink is detected\n\n            # Check for wink detection and perform scroll action\n            check_wink_detection(shape)\n\n        # Display the image in the Streamlit app\n        image_placeholder.image(image, channels="BGR", use_column_width=True)\n\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n    # Release the camera capture\n    cap.release()\n    cv2.destroyAllWindows()\n\n# Streamlit app\nst.title("Eye Tracking App")\n\n# Run eye tracking\nif st.button("Start Eye Tracking"):\n    run_eye_tracking()\n')


# In[ ]:


get_ipython().system('streamlit run hello_world.py')
#%run hello_world.py

